{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scrape_zappos_data.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iio1_sxMWXzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install libraries\n",
        "! pip install BeautifulSoup4 s3fs requests tqdm skafos turicreate==5.6 pandas numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ4CdpxwWXzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import libraries\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import s3fs\n",
        "import urllib\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import coremltools \n",
        "import turicreate as tc\n",
        "from tqdm import tqdm\n",
        "from skafos import models\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8SZltv5WXzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some constants \n",
        "DATASET_ID_FORMAT = \"%Y%m%d%H%M%S\"\n",
        "base_url = \"https://www.zappos.com/\"\n",
        "womens_boots_url = base_url + \"women-boots/CK_XARCz1wHAAQHiAgMBAhg.zso\"\n",
        "n = 1  # track page number\n",
        "\n",
        "# Model constants\n",
        "retrain_threshold = 1\n",
        "model_name = \"ImageSimilarity\"\n",
        "coreml_model_name = model_name + \".mlmodel\"\n",
        "app_name = \"BootFinder\"\n",
        "\n",
        "# Data containers\n",
        "new_meta_data = []\n",
        "\n",
        "# S3 Filesystem\n",
        "s3 = s3fs.S3FileSystem(anon=False)\n",
        "bucket = \"skafos.bootfinder/\"\n",
        "img_path = '/boot_images/'\n",
        "meta = 'boots_meta_data.json'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMFf74_T7Ti1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions\n",
        "def upload_boots_to_s3(s3, meta_data):\n",
        "  # Upload new data to s3\n",
        "  dataset = datetime.now().strftime(DATASET_ID_FORMAT)\n",
        "  # Write the meta data dictionary locally and to s3\n",
        "  print(\"Writing meta data json file to local and s3\")\n",
        "  with open(meta, 'w') as f:\n",
        "    f.write(json.dumps(meta_data))\n",
        "  with s3.open(bucket + dataset + '/' + meta, 'w') as f:\n",
        "    f.write(json.dumps(meta_data))\n",
        "  # Write new images to s3\n",
        "  print(\"Uploading images to s3\", flush=True)\n",
        "  for img in tqdm(meta_data):\n",
        "    with s3.open(bucket + dataset + img_path + img['boot_id'], 'wb') as f:\n",
        "      f.write(requests.get(img['image_source']).content)  \n",
        "  return dataset\n",
        "\n",
        "\n",
        "def retrain_image_similarity_model(s3, dataset, meta_data):\n",
        "  print(\"\\nRetraining image similarity model!\", flush=True)\n",
        "  # Pull in boot images from S3 \n",
        "  _local_dir = 'boot_images'\n",
        "  if not os.path.exists(_local_dir):\n",
        "    os.makedirs(_local_dir)\n",
        "  # List out boots\n",
        "  boots = s3.ls(\"s3://skafos.bootfinder/{}/boot_images/\".format(dataset))\n",
        "  # Download boot images from s3\n",
        "  print(\"Downloading boot images to train\", flush=True)\n",
        "  for b in tqdm(boots): \n",
        "    _local_file = \"/\".join(b.split(\"/\")[-1:])\n",
        "    _local_path = _local_dir + \"/\" + _local_file\n",
        "    s3.get(\"s3://\" + b, _local_path)\n",
        "  # Create boot SFrame\n",
        "  boot_data  = tc.image_analysis.load_images('boot_images')\n",
        "  boot_data = boot_data.add_row_number()\n",
        "  # Build image similarity model using SqueezeNet, as it is smaller than Resnet\n",
        "  model = tc.image_similarity.create(boot_data, model=\"squeezenet_v1.1\")\n",
        "  return model\n",
        "\n",
        "       \n",
        "def upload_model_to_s3(s3, dataset, coreml_model_name, new_skews):\n",
        "  print(\"\\nUploading trained model to s3\", flush=True)\n",
        "  with open(coreml_model_name, 'rb') as model_data:\n",
        "    with s3.open(bucket + dataset + '/' + coreml_model_name, 'wb') as f:\n",
        "      f.write(model_data.read())\n",
        "  print(\"Uploading list of new skews included in the model since last run\")\n",
        "  with s3.open(bucket + dataset + '/new_skews.json', 'w') as f:\n",
        "    f.write(json.dumps(new_skews))\n",
        "\n",
        "    \n",
        "def upload_model_to_skafos(dataset, new_skews):\n",
        "  # Uses API Token, Org Name Env Vars\n",
        "  print(\"\\nUploading trained model and meta data to skafos\", flush=True)\n",
        "  res = models.upload_version(\n",
        "      files=[coreml_model_name, meta],\n",
        "      description=\"Dataset: {}. {} new boots included.\".format(dataset, len(new_skews)),\n",
        "      model_name=model_name,\n",
        "      app_name=app_name\n",
        "  )\n",
        "  return res\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZaBuNBcXAiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Step 1: Ingest New Zappos Boots Data ##\n",
        "# Parse data based on the structure of the page\n",
        "# We are going to create a list of dictionaries that we will write to file\n",
        "while True:\n",
        "    print(\"Processing page {} of boots\".format(n), flush=True)\n",
        "    url = womens_boots_url + \"?p={}\".format(n)\n",
        "    page = urllib.request.urlopen(url)\n",
        "    soup = BeautifulSoup(page, 'html.parser')\n",
        "    all_links = soup.find_all('a')\n",
        "    valid_page_links = 0\n",
        "    # Process data\n",
        "    for link in all_links:\n",
        "        aria_label = link.get('aria-label')\n",
        "        buy_link = link.get('href')\n",
        "        if aria_label and link.img and buy_link:\n",
        "            # Sort to make sure that what we get back has a price, ensuring it is a boot we want to include\n",
        "            if aria_label.find('$') != -1:\n",
        "                valid_page_links += 1\n",
        "                # Use the image name as the id\n",
        "                _id = link.img['src'].split('/')[-1] \n",
        "                # Check for a rating in the label\n",
        "                rating = re.search(\"Rated (.*?)\\.\", aria_label)\n",
        "                if rating:\n",
        "                    rating = rating.group(1)\n",
        "                # Append items to the meta dictionary one by one (zero indexed)\n",
        "                new_meta_data.append({\n",
        "                    'boot_id': _id,\n",
        "                    'boot_name': aria_label.split('. By')[0],\n",
        "                    'brand': re.search(\"By (.*?) \\$\", aria_label).group(1).strip('.'),\n",
        "                    'price': \"$\" + re.search(\"\\$(.*?) \", aria_label).group(1).strip('.'),\n",
        "                    'style': re.search(\"Style: (.*?)\\.\", aria_label).group(1),\n",
        "                    'rating': rating, \n",
        "                    'image_source': link.img['src'],\n",
        "                    'buy_link': base_url + buy_link.strip('/')\n",
        "                })\n",
        "    if valid_page_links == 0:\n",
        "        print(\"..No more valid boot links found. Done ingesting.\", flush=True)\n",
        "        break\n",
        "    n += 1\n",
        "\n",
        "# Sort and organize meta data and newly collected boot ids\n",
        "new_meta_data = sorted(metnew_meta_dataa_data, key=lambda k: k['boot_id'])\n",
        "new_boot_ids = set([boot['boot_id'] for boot in new_meta_data])\n",
        "assert len(new_meta_data) == len(new_boot_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0xOVMK8ZXj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Step 2: Check to see how many new boots we have since last ingest ##\n",
        "previous_dataset = sorted([file.split('/')[1] for file in s3.ls(bucket)], key=lambda k: datetime.strptime(k, DATASET_ID_FORMAT), reverse=True)[0]\n",
        "previous_boot_ids = set(img.split('/')[-1] for img in s3.ls(bucket + previous_dataset + img_path))\n",
        "\n",
        "# Extract the new skews since last ingest\n",
        "new_skews = [boot['image_source'] for skew in new_boot_ids.difference(previous_boot_ids) for boot in new_meta_data if skew == boot['boot_id']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF_1X2tL6Qd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Step 3: If we have enough new ones, retrain the model ##\n",
        "print(\"Found {} new boots since last ingest!\".format(len(new_skews)), flush=True)\n",
        "if len(new_skews) >= retrain_threshold:\n",
        "    print(\"New boots: {}\".format(new_skews), flush=True)\n",
        "\n",
        "    # Upload data to s3\n",
        "    new_dataset = upload_boots_to_s3(s3, new_meta_data)\n",
        "\n",
        "    # Retrain model\n",
        "    new_model = retrain_image_similarity_model(s3, new_dataset, new_meta_data)\n",
        "\n",
        "    # Convert model to coreml\n",
        "    print(\"\\nConverting model to coreml format\", flush=True)\n",
        "    new_model.export_coreml(coreml_model_name)\n",
        "\n",
        "    # Upload model to s3\n",
        "    upload_model_to_s3(s3, new_dataset, coreml_model_name, new_skews)\n",
        "\n",
        "    # Upload model to Skafos\n",
        "    res = upload_model_to_skafos(new_dataset, new_skews)\n",
        "    print(res)\n",
        "else:\n",
        "    # Do nothing - close out\n",
        "    sys.exit(\"Not enough new boots.. Packing up and going home.\")\n",
        "\n",
        "\n",
        "print(\"\\nDone.\", flush=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}